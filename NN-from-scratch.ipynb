{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN from Scratch\n",
    "\n",
    "\n",
    "Neural Network Introduction\n",
    "- A neural network is a series of algorithms that attempt to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.\n",
    "- made up of nodes\n",
    "- Node: \n",
    "\n",
    "\n",
    "Various Fields of Neural Network \n",
    "\n",
    "![image](./images-for-ipynb/Fields%20of%20AI.png)\n",
    "\n",
    "\n",
    "Types: \n",
    "- supervised = re-established and labeled data that can be used fÃ¶r training \n",
    "- unsupervised = the machine finds structure in data without knowing the labels/classes ahead of time.\n",
    "\n",
    "\n",
    "Example of Supervised Learning: \n",
    " - lets say you have a smart thermostat device that measures temperature and humidity every 10 minutes. Here, temperature and humidity are `features`\n",
    " - you have differentiated them into \"normal\" where the thermostat was running and \"failure\" when the thermostat failed to work.\n",
    "- values of features = `samples`\n",
    "- sample are fed to the NN to train them. \n",
    "    - will fit desired output from the i/ps or to predict based on the them during the inference phase.\n",
    "- 'normal' and 'failure' are `classification` or `labels`\n",
    "- this is an example of Classification task. \n",
    "\n",
    "\n",
    "Classification Task\n",
    "- cat or dog\n",
    "- male or female\n",
    "\n",
    "Regression Task \n",
    "- predit the score of a student based on his/her attendance. \n",
    "- risk calculation\n",
    "\n",
    "Unsupervised Learning\n",
    "    - reinforcement learning\n",
    "        - making an autonomus car. \n",
    "    - semi-supervised learning\n",
    "\n",
    "Covered Topics: \n",
    "- Classification form Scratch\n",
    "- Regression from Scratch\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History of NN\n",
    "\n",
    "Neural networks were conceived in the 1940s, but figuring out how to train them remained a mystery for 20 years. The concept of `backpropagation` (explained later) came in the 1960s, but neural networks still did not receive much attention until they started winning competitions in 2010. Since then, neural networks have been on a meteoric rise due to their sometimes seemingly magical ability to solve problems previously deemed unsolvable, such as image captioning, language translation, audio and video synthesis, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neural Network? \n",
    "\n",
    "- insprired by organic brain, translated to the computer. (trying to model a human brain (not accurate but is still effective))\n",
    "\n",
    "\n",
    "Comparision of Biological neuron to an artificial neurons\n",
    "![image.png](./images-for-ipynb/Bilological%20Neurons%20vs%20Artificaial%20Neuron.png)\n",
    "\n",
    "Here, \n",
    "    - b is the bias\n",
    "    - f is the activation funtion of choice. \n",
    "\n",
    "\n",
    "\n",
    "- single neuron is useless \n",
    "- but hundreds or thousands of neurons are very powerful. \n",
    "- here the interconnectivity produces relationships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a neural netwrok with 3 hidden layers of 16 neurons each\n",
    "\n",
    "![image.png](./images-for-ipynb/Example%20of%20a%20neural%20network%20with%203%20hidden%20layers%20of%2016%20neurons%20each.png)\n",
    "\n",
    "- NN ara considersd to be \"black boxes\" in that we often have no idea why they reach the conclusion they do.  We do understand how they do it though. \n",
    "\n",
    "Dense layer/Fully Connected Layer: \n",
    "- each neuron of a given layer is connected to every neurons of the next layer, which means that its output value becomes an input for the next neurons. \n",
    "- Each connection has a `weight` associated with it. : trainable facter\n",
    "- weight gets multiplied by the input value. `input*weights*\n",
    "- bias: trainable factor is added. => purpose is to offset the output +vely or -vely.\n",
    "\n",
    "The concept of weights and biases can be thought of as \"knobs\" that we can tune to fit our model to data.\n",
    "\n",
    "\"why not just have biases or just weights?\"\n",
    "- they tune the network in different ways. \n",
    "    - weights are multiplied, so they will only change the mangnitude or even completely flip the sign from -ve to positive, or vice versa. \n",
    "    -  `Output = weight*input + bias` \n",
    "    - similar to  y=mx+c\n",
    "    - ![image](./images-for-ipynb/Analogy%20of%20wight%20and%20bias.png)\n",
    "    \n",
    "    |![image](./images-for-ipynb/weight-and-bias1.png)|![image](./images-for-ipynb/weight-and-bias2.png)|\n",
    "    |--------|---------|\n",
    "    |![image](./images-for-ipynb/weight-and-bias3.png)|![image](./images-for-ipynb/weight-and-bias4.png)|\n",
    "\n",
    "    Weights and bias help to impact the o/p of the neurons, but in different ways. It will be more clear when we cover activaation funcitons. \n",
    "\n",
    "As a very general overview, the step function meant to mimic a neuron in the brain, either \"firing\"\n",
    "like an on-off switch. In programming, an on-off switch as a function would be called a\n",
    "or step function because it looks like a step if we graph it. \n",
    "\n",
    "![image](./images-for-ipynb/graph-of-step-function.png)\n",
    "\n",
    "For a single neuron, \n",
    "\n",
    "![image](./images-for-ipynb/single%20neruon.png)\n",
    "`output = sum(inputs*weights) + bias`\n",
    "\n",
    "To mimic the theory of brain cell firing or not firing,\n",
    "- if output > 0 => neuron fires. so output for the activation function is 1\n",
    "- if output < 0 => neuron does not fire. so the output for the activation funciton is 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So mathmatically, for a single neuron\n",
    "\n",
    "`z = bias + x1*w1 + x2*w2 + ... + xn*wn`\n",
    "\n",
    "where \n",
    "    x1, x2, x3.. xn are the inputs and   \n",
    "    w1, w2, w3 ... wn are the weights associated with branch\n",
    "\n",
    "which is as we have written above, only difference is that \"output\" is replaced by z: \n",
    "\n",
    "`z = sum(inputs*weights)`\n",
    "\n",
    "And activation funciton, \n",
    "\n",
    "TODO: REPALCE THE FORUMA IN PHOTO WITH LATEX EQUATION\n",
    "\n",
    "`y = activation(z)`\n",
    "\n",
    "this is also sometimes written as\n",
    "\n",
    "`y = g(z)` \n",
    "\n",
    "\n",
    "![image](./images-for-ipynb/step-function%20formula.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can use a step function for your activation function, we tend to use something slightly more advanced. Neural networks of today tend to use more informative activation functions (rather than a step function), such as the Rectified Linear (ReLU) activation function, which we will cover in-depth later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with 2 hidden layers\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
